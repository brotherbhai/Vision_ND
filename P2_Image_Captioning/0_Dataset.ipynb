{"nbformat":4,"nbformat_minor":0,"metadata":{"anaconda-cloud":{},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"},"colab":{"name":"0_Dataset.ipynb","provenance":[]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"Idj49doE-bj-","colab_type":"text"},"source":["# Computer Vision Nanodegree\n","\n","## Project: Image Captioning\n","\n","---\n","\n","The Microsoft **C**ommon **O**bjects in **CO**ntext (MS COCO) dataset is a large-scale dataset for scene understanding.  The dataset is commonly used to train and benchmark object detection, segmentation, and captioning algorithms.  \n","\n","![Sample Dog Output](images/coco-examples.jpg)\n","\n","You can read more about the dataset on the [website](http://cocodataset.org/#home) or in the [research paper](https://arxiv.org/pdf/1405.0312.pdf).\n","\n","In this notebook, you will explore this dataset, in preparation for the project.\n","\n","## Step 1: Initialize the COCO API\n","\n","We begin by initializing the [COCO API](https://github.com/cocodataset/cocoapi) that you will use to obtain the data."]},{"cell_type":"code","metadata":{"id":"rG2pjehsLimE","colab_type":"code","outputId":"2c89f4a4-73e0-4c99-82ee-cc8bb11ee074","executionInfo":{"status":"ok","timestamp":1574446489285,"user_tz":-480,"elapsed":859,"user":{"displayName":"nazrul ismail","photoUrl":"","userId":"07513880639336204123"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["from google.colab import drive \n","drive.mount(\"/content/drive\")\n","%cd drive/'My Drive'/CVND/CVND_projects"],"execution_count":38,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","[Errno 2] No such file or directory: 'drive/My Drive/CVND/CVND_projects'\n","/root\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"jjFSB-neMG7h","colab_type":"code","outputId":"3800caff-b71d-48e1-e228-1b153b191e05","executionInfo":{"status":"ok","timestamp":1574444268119,"user_tz":-480,"elapsed":1113,"user":{"displayName":"nazrul ismail","photoUrl":"","userId":"07513880639336204123"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["%cd ../content"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/content\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1SMkB2FTLsYK","colab_type":"code","outputId":"614d6d9e-6154-4116-b9c3-fcdbb2450596","executionInfo":{"status":"ok","timestamp":1574446496467,"user_tz":-480,"elapsed":903,"user":{"displayName":"nazrul ismail","photoUrl":"","userId":"07513880639336204123"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["%cd P2_Image_Captioning"],"execution_count":39,"outputs":[{"output_type":"stream","text":["[Errno 2] No such file or directory: 'P2_Image_Captioning'\n","/root\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"h_DOcKiOLzHV","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":63},"outputId":"3a3b9acc-8202-4809-d0b1-46ae60b50268","executionInfo":{"status":"ok","timestamp":1574446500120,"user_tz":-480,"elapsed":3785,"user":{"displayName":"nazrul ismail","photoUrl":"","userId":"07513880639336204123"}}},"source":["!cp -r cocoapi ~"],"execution_count":40,"outputs":[{"output_type":"stream","text":["cp: 'cocoapi' and '/root/cocoapi' are the same file\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"R8S5-JnFL5H5","colab_type":"code","outputId":"2b58898d-a6b3-4157-aa3c-5adf4c16ee20","executionInfo":{"status":"ok","timestamp":1574446509484,"user_tz":-480,"elapsed":3761,"user":{"displayName":"nazrul ismail","photoUrl":"","userId":"07513880639336204123"}},"colab":{"base_uri":"https://localhost:8080/","height":63}},"source":["!ls ~/cocoapi/annotations/"],"execution_count":41,"outputs":[{"output_type":"stream","text":["captions_val2014.json  instances_val2014.json\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"cx-Wfu2g-bkC","colab_type":"code","outputId":"6297bdd8-2560-4870-ed8a-82601ec169a8","executionInfo":{"status":"ok","timestamp":1574446530266,"user_tz":-480,"elapsed":7321,"user":{"displayName":"nazrul ismail","photoUrl":"","userId":"07513880639336204123"}},"colab":{"base_uri":"https://localhost:8080/","height":153}},"source":["import os\n","import sys\n","sys.path.append('~')\n","from pycocotools.coco import COCO\n","\n","# initialize COCO API for instance annotations\n","dataDir = 'cocoapi'\n","dataType = 'val2014'\n","instances_annFile = os.path.join(dataDir, 'annotations/instances_{}.json'.format(dataType))\n","coco = COCO(instances_annFile)\n","\n","# initialize COCO API for caption annotations\n","captions_annFile = os.path.join(dataDir, 'annotations/captions_{}.json'.format(dataType))\n","coco_caps = COCO(captions_annFile)\n","\n","# get image ids \n","ids = list(coco.anns.keys())"],"execution_count":43,"outputs":[{"output_type":"stream","text":["loading annotations into memory...\n","Done (t=4.64s)\n","creating index...\n","index created!\n","loading annotations into memory...\n","Done (t=0.30s)\n","creating index...\n","index created!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"hGr9fI7qLSVG","colab_type":"code","colab":{}},"source":["!ls"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"99fFvu5o-bkL","colab_type":"text"},"source":["## Step 2: Plot a Sample Image\n","\n","Next, we plot a random image from the dataset, along with its five corresponding captions.  Each time you run the code cell below, a different image is selected.  \n","\n","In the project, you will use this dataset to train your own model to generate captions from images!"]},{"cell_type":"code","metadata":{"id":"s5A9hDYW-bkN","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"6182f2ee-75be-4da1-a393-59c74555e9f0"},"source":["import numpy as np\n","import skimage.io as io\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","# pick a random image and obtain the corresponding URL\n","ann_id = np.random.choice(ids)\n","img_id = coco.anns[ann_id]['image_id']\n","img = coco.loadImgs(img_id)[0]\n","url = img['coco_url']\n","\n","# print URL and visualize corresponding image\n","print(url)\n","I = io.imread(url)\n","plt.axis('off')\n","plt.imshow(I)\n","plt.show()\n","\n","# load and display captions\n","annIds = coco_caps.getAnnIds(imgIds=img['id']);\n","anns = coco_caps.loadAnns(annIds)\n","coco_caps.showAnns(anns)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["http://images.cocodataset.org/val2014/COCO_val2014_000000390718.jpg\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"mfIzFrjQ-bkT","colab_type":"text"},"source":["## Step 3: What's to Come!\n","\n","In this project, you will use the dataset of image-caption pairs to train a CNN-RNN model to automatically generate images from captions.  You'll learn more about how to design the architecture in the next notebook in the sequence (**1_Preliminaries.ipynb**).\n","\n","![Image Captioning CNN-RNN model](images/encoder-decoder.png)"]}]}