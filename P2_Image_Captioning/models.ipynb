{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"models.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"Rgt9YjADlHQk","colab_type":"code","colab":{}},"source":["import torch \n","import torch.nn as nn\n","import torchvision.models as models \n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PfKeIL9AlSLe","colab_type":"code","colab":{}},"source":["class EncoderCNN(nn.Module):\n","    def __init__(self, embed_size):\n","        super(EncoderCNN, self).__init__()\n","        self.embed_size = embed_size\n","        resnet = models.resnet50(pretrained=True)\n","        for param in resnet.parameters():\n","            param.requires_grad_(False)\n","        resnet_modules = list(resnet.children())[:-1] \n","        self.backbone = nn.Sequential(*resnet_modules)\n","        self.tail = nn.Linear(in_features=resnet.fc.in_features, \n","                                    out_features=embed_size)\n","    def forward(self, x):\n","        x = self.backbone(x)\n","        x = x.view(x.size(0), -1)\n","        x = self.tail(x)\n","        return x\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"LXYvhNvFmvZm","colab_type":"code","colab":{}},"source":["class DecoderRNN(nn.Module):\n","    def __init__(self,  embed_size, hidden_size, vocab_size, dropout_prob=.5, \n","                 num_layers=2, \n","                 ):\n","        super(DecoderRNN, self).__init__()\n","        self.n_layers = num_layers\n","        self.vocab_size = vocab_size\n","        self.hidden_size = hidden_size\n","        self.caption_embeddings = nn.Embedding(vocab_size, embed_size)\n","        self.drop_prob = dropout_prob\n","\n","        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, \n","                            dropout= self.drop_prob, batch_first=True)\n","        self.dropout = nn.Dropout(self.drop_prob)\n","\n","        self.fc = nn.Linear(in_features=hidden_size, \n","                            out_features=vocab_size)\n","        \n","        self.init_weights()\n","\n","\n","    def forward(self, features, captions):\n","        captions = captions[:, :-1]\n","        captions = self.caption_embeddings(captions)\n","        \n","        #concatenate features extracted and captions \n","        concat = torch.cat((features.unsqueeze(1), captions), 1)\n","\n","        output, hidden = self.lstm(concat)\n","        output = self.dropout(output)\n","        output = self.fc(output)\n","\n","        return output\n","    \n","    def sample(self, inputs, states=None, max_len=20):\n","        \" accepts pre-processed image tensor (inputs) and returns predicted sentence (list of tensor ids of length max_len) \"\n","        tokens = []\n","      \n","        #Forward prop through remaining states until reach Captions's max_len\n","        for ii in range(max_len):\n","            output, states = self.lstm(inputs, states)\n","            #print(f\"Before FC: {output.size()}\")\n","            output = self.fc(output.squeeze(1))\n","            #print(f\"After FC: {output.size()}\")\n","            _, token = output.max(1) #This will return the caption token \n","            tokens.append(token.item())\n","\n","            #input to the next timestep \n","            inputs = self.caption_embeddings(token) #word2idx\n","            inputs = inputs.unsqueeze(1)\n","\n","        return tokens\n","\n","    def init_weights(self):\n","        \"\"\"init for fully connected layer\"\"\"\n","        self.fc.bias.data.fill_(0)\n","        self.fc.weight.data.uniform_(-1, 1)\n","\n","    def init_hidden(self):\n","        weight = next(self.parameters()).data        \n","        return (weight.new(self.n_layers, self.vocab_size, self.hidden_size).zero_(),\n","                weight.new(self.n_layers, self.vocab_size, self.hidden_size).zero_())\n","        "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BubZDin0_N6w","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}